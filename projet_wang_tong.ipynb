{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numpy,Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') \n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import missingno as msno\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/jeandedieunyandwi/lending-club-dataset?datasetId=608703&sortBy=voteCount\n",
    "loans = pd.read_csv('lending_club_loan_two.csv' , encoding='latin-1') \n",
    "loans.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = loans.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['term',\n",
    " 'emp_title',\n",
    " 'emp_length',\n",
    " 'home_ownership',\n",
    " 'verification_status',\n",
    " 'purpose',\n",
    " 'title',\n",
    " 'initial_list_status',\n",
    " 'application_type',\n",
    " ]\n",
    "\n",
    "for col in cat_cols:\n",
    "    loans[col] = loans[col].astype(\"category\")\n",
    "    \n",
    "loans[\"issue_d\"] = pd.to_datetime(loans[\"issue_d\"]).astype('datetime64[ns]')\n",
    "loans[\"earliest_cr_line\"] = pd.to_datetime(loans[\"earliest_cr_line\"]).astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe(include=['category']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.isnull().sum(axis=0).sort_values(ascending=False)/float(len(loans)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = loans.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "loans[cat_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = loans.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "num_cols.remove(\"pub_rec_bankruptcies\")\n",
    "num_cols.remove(\"mort_acc\")\n",
    "num_cols.remove(\"pub_rec\")\n",
    "original_obs = loans.shape[0]\n",
    "\n",
    "for col in num_cols:\n",
    "    q25 = np.nanquantile(loans[col], 0.25)\n",
    "    q75 = np.nanquantile(loans[col], 0.75)\n",
    "    IQR = q75 - q25\n",
    "    drop_ind = (loans[col]< q25 - 1.5*IQR) | (loans[col]> q75 + 1.5*IQR)\n",
    "    print(\"Percentage of outliers in column \", col ,\"is : \", np.round(loans[drop_ind][col].count()*100/loans[col].count(), decimals=3), \"  || n_obs =\", loans[drop_ind][col].count(),\"||  max =\", q75 + 1.5*IQR )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_od=loans.copy()\n",
    "for col in num_cols:\n",
    "    q25 = np.nanquantile(loans_od[col], 0.25)\n",
    "    q75 = np.nanquantile(loans_od[col], 0.75)\n",
    "    med = np.nanquantile(loans_od[col], 0.5)\n",
    "    IQR = q75 - q25\n",
    "\n",
    "    drop_ind = (loans_od[col]< q25 - 1.5*IQR)\n",
    "    loans_od = loans_od[~drop_ind]\n",
    "    drop_ind = (loans_od[col]> q75 + 1.5*IQR)\n",
    "    loans_od = loans_od[~drop_ind]\n",
    "    \n",
    "print(\"Percentage of values dropped : \", 100 - loans_od.shape[0]*100/original_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "cycol = cycle('bgrcm')\n",
    "\n",
    "num_cols = ['loan_amnt',\n",
    " 'int_rate',\n",
    " 'installment',\n",
    " 'annual_inc',\n",
    " 'dti',\n",
    " 'open_acc',\n",
    " 'revol_bal',\n",
    " 'revol_util',\n",
    " 'total_acc']\n",
    "\n",
    "fig, ax = plt.subplots(len(num_cols),2, figsize=(16,40))\n",
    "\n",
    "for i in range(len(num_cols)):\n",
    "    color_next = next(cycol)\n",
    "    sns.distplot(loans_od[num_cols[i]], ax=ax[i,0], color=color_next)\n",
    "    ax[i,0].set_title(num_cols[i])\n",
    "    ax[i,0].set_xlabel('')\n",
    "    sns.boxplot(loans_od[num_cols[i]], width = 0.3, ax=ax[i,1], color=color_next)\n",
    "    ax[i,1].set_title(num_cols[i]+str(\"-Boxplot\"))\n",
    "    ax[i,1].set_xlabel('')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['loan_status'] = loans['loan_status'].map({'Fully Paid':0,'Charged Off':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(14,7))\n",
    "sns.countplot(x='loan_status',data=loans,ax=axs[0])\n",
    "axs[0].set_title(\"Frequency of each Loan Status\")\n",
    "loans['loan_status'].value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\n",
    "axs[1].set_title(\"Percentage of each Loan status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target V & categorical V\n",
    "low_cat_cols = ['term','grade','emp_length','home_ownership','verification_status','purpose', 'initial_list_status', 'application_type']\n",
    "for col in low_cat_cols:\n",
    "    Gender=pd.crosstab(loans[col],loans[\"loan_status\"])\n",
    "    Gender.div(Gender.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Outlier Treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    q25 = np.nanquantile(loans[col], 0.25)\n",
    "    q75 = np.nanquantile(loans[col], 0.75)\n",
    "    med = np.nanquantile(loans[col], 0.50)\n",
    "    IQR = q75 - q25\n",
    "    \n",
    "    replace_ind = (loans[col]< q25 - 1.5*IQR)\n",
    "    loans.loc[replace_ind,col] = med\n",
    "    replace_ind = (loans[col]> q75 + 1.5*IQR)\n",
    "    loans.loc[replace_ind,col] = med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MIssing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.isnull().sum(axis=0).sort_values(ascending=False)/float(len(loans)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### emp_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['emp_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['emp_length'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### public_rec_bankruptcies, revol_util\n",
    "The columns 'public_rec_bankruptcies' and 'revol_util' have very few missing values. We replace those with medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(axis = 0, how= 'any', subset = ['pub_rec_bankruptcies'])\n",
    "loans[\"pub_rec_bankruptcies\"] = loans[\"pub_rec_bankruptcies\"].fillna(loans[\"pub_rec_bankruptcies\"].median())\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].fillna(loans[\"revol_util\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mort_acc\n",
    "\n",
    "We are now left with mort_acc. We fill up using the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['mort_acc'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['mort_acc'] = loans['mort_acc'].fillna(0.00000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.isnull().sum(axis=0).sort_values(ascending=False)/float(len(loans)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert01(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "loans['pub_rec'] = loans['pub_rec'].apply(lambda x: convert01(x))\n",
    "loans['pub_rec_bankruptcies'] = loans['pub_rec_bankruptcies'].apply(lambda x: convert01(x))\n",
    "loans['mort_acc'] = loans['mort_acc'].apply(lambda x: convert01(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['address'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['address'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### issue_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['issue_d'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['issue_d'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### earliest_cr_line\n",
    "We also extract the earliest issued credit line year out of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loans['earliest_cr_line'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['earliest_cr_line'] = loans['earliest_cr_line'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loans['earliest_cr_line'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Home Ownership\n",
    "We see that 'OTHER', 'NONE' & 'ANY' are very few in number. We could club all three into one category 'Other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_other(x):\n",
    "    if x in ['ANY','OTHER','NONE']:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "loans['home_ownership'] = loans['home_ownership'].apply(lambda x : make_other(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grade and Sub Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=['sub_grade'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping_dict = {\n",
    "    \"grade\":{\n",
    "        \"A\": 1,\n",
    "        \"B\": 2,\n",
    "        \"C\": 3,\n",
    "        \"D\": 4,\n",
    "        \"E\": 5,\n",
    "        \"F\": 6,\n",
    "        \"G\": 7\n",
    "    }\n",
    "}\n",
    "\n",
    "loans = loans.replace(mapping_dict) \n",
    "loans['grade'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE / Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ['term', 'home_ownership', 'verification_status','purpose', 'application_type','initial_list_status']\n",
    "\n",
    "loans = pd.get_dummies(loans, columns=dummies, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = loans.select_dtypes(include=['int64','float64']).columns\n",
    "len(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('loan_status') \n",
    "loans_ml_df = loans \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc =StandardScaler() \n",
    "loans_ml_df[col] =sc.fit_transform(loans_ml_df[col])\n",
    "loans_ml_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_feature = list(loans_ml_df.columns)\n",
    "x_feature.remove('loan_status')\n",
    "x_val = loans_ml_df[x_feature]\n",
    "y_val = loans_ml_df['loan_status']\n",
    "len(x_feature) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_ml_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrapper approacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "rfe = RFE(model, n_features_to_select=20,step=1) \n",
    "rfe = rfe.fit(x_val, y_val)\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_filter = x_val.columns[rfe.support_]\n",
    "col_filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(loans_ml_df[col_filter].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['total_acc', 'verification_status_Verified','application_type_JOINT','int_rate']\n",
    "col_new = col_filter.drop(drop_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(loans_ml_df[col_new].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embedded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "names = loans_ml_df[col_new].columns\n",
    "clf=RandomForestClassifier(n_estimators=10,random_state=123)\n",
    "clf.fit(x_val[col_new], y_val) \n",
    "names, clf.feature_importances_\n",
    "for feature in zip(names, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "feat_names = names\n",
    "indices = np.argsort(importances)[::-1]\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "plt.title(\"Feature importances by RandomTreeClassifier\")\n",
    "plt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\n",
    "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_col = ['application_type_INDIVIDUAL', 'purpose_educational','purpose_renewable_energy','purpose_wedding','purpose_house','purpose_medical','purpose_small_business']\n",
    "drop_col = ['application_type_INDIVIDUAL', 'purpose_educational','purpose_renewable_energy']\n",
    "col_new = col_new.drop(drop_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = loans_ml_df.drop('loan_status',1)\n",
    "y = loans_ml_df[\"loan_status\"]\n",
    "\n",
    "n_sample = y.shape[0]\n",
    "n_pos_sample = y[y == 0].shape[0]\n",
    "n_neg_sample = y[y == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n",
    "\n",
    "n_sample = y_train.shape[0]\n",
    "n_pos_sample = y_train[y_train == 0].shape[0]\n",
    "n_neg_sample = y_train[y_train == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape', X_train.shape[1])\n",
    "\n",
    "n_sample = y_test.shape[0]\n",
    "n_pos_sample = y_test[y_test == 0].shape[0]\n",
    "n_neg_sample = y_test[y_test == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape',  X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = loans_ml_df[col_new]\n",
    "y_new = loans_ml_df[\"loan_status\"]\n",
    "\n",
    "n_sample = y_new.shape[0]\n",
    "n_pos_sample = y_new[y_new == 0].shape[0]\n",
    "n_neg_sample = y_new[y_new == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape', X_new.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size = 0.3, random_state = 0) \n",
    "\n",
    "n_sample = y_train_new.shape[0]\n",
    "n_pos_sample = y_train_new[y_train_new == 0].shape[0]\n",
    "n_neg_sample = y_train_new[y_train_new == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape', X_train_new.shape[1])\n",
    "\n",
    "n_sample = y_test_new.shape[0]\n",
    "n_pos_sample = y_test_new[y_test_new == 0].shape[0]\n",
    "n_neg_sample = y_test_new[y_test_new == 1].shape[0]\n",
    "print('Total:{}; 0 :{:.2%}; 1 :{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('shape',  X_test_new.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "### Model Building + Validation and Evaluation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "\n",
    "param_grid = {'C': [0.01,0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "kflod = StratifiedKFold(n_splits=10, shuffle = True,random_state=0)\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "grid_search = GridSearchCV(model,param_grid, cv= kflod) \n",
    "grid_search.fit(X_train_new, y_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_) \n",
    "best = np.argmax(results.mean_test_score.values)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.5f}\".format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_lr_new = LogisticRegression(C=0.01,class_weight='balanced')\n",
    "mode_lr_new.fit(X_train_new,y_train_new)\n",
    "y_pred_new = mode_lr_new.predict(X_test_new)\n",
    "print(\"Test set accuracy score: {:.5f}\".format(accuracy_score(y_test_new, y_pred_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_new, y_pred_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc1 = roc_auc_score(y_test_new, y_pred_new)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train_new, y_train_new)\n",
    "print('apres SMOTE ')\n",
    "n_sample = y_train_sm.shape[0]\n",
    "n_pos_sample = y_train_sm[y_train_sm == 0].shape[0]\n",
    "n_neg_sample = y_train_sm[y_train_sm == 1].shape[0]\n",
    "print('Total: {}; 0 : {:.2%}; 1 : {:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = X_train_sm.copy()\n",
    "y_train_rf = y_train_sm.copy()\n",
    "X_test_rf = X_test_new.copy()\n",
    "y_test_rf = y_test_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_lr_sm = LogisticRegression(C=0.01)\n",
    "mode_lr_sm.fit(X_train_sm,y_train_sm)\n",
    "y_pred_sm = mode_lr_sm.predict(X_test_new)\n",
    "print(\"Test set accuracy score: {:.5f}\".format(accuracy_score(y_test_new, y_pred_sm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_new, y_pred_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc2 = roc_auc_score(y_test_new, y_pred_sm)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_rf, y_train_rf)\n",
    "score1 = rf.score(X_test_rf,y_test_rf)\n",
    "print(score1)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "proba = rf.predict_proba(X_test_rf)\n",
    "score2 = roc_auc_score(y_test,proba[:,1])\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score3 = cross_val_score(rf,X_train_rf,y_train_rf,scoring='accuracy',cv = 5)\n",
    "print(score3)\n",
    "print(score3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test_rf)\n",
    "print(rf.score(X_test_rf,y_test_rf))\n",
    "print(classification_report(y_test_rf, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estimator = {'n_estimators':range(50,300,50)}\n",
    "gs1 = GridSearchCV(estimator = rf,param_grid = num_estimator,scoring='roc_auc',cv = 5)\n",
    "gs1.fit(X_train_rf,y_train_rf)\n",
    "print(gs1.best_estimator_)\n",
    "print(gs1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdepth = {'max_depth':range(3,10,1)}\n",
    "gs2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = 250),param_grid = maxdepth,scoring = 'roc_auc',cv = 3)\n",
    "gs2.fit(X_train_rf,y_train_rf)\n",
    "print(gs2.best_estimator_)\n",
    "print(gs2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsamples = {'min_samples_split':range(2,14,2)}\n",
    "gs3 = GridSearchCV(estimator = RandomForestClassifier(max_depth=9, n_estimators=250),param_grid = minsamples,scoring = 'roc_auc',cv = 3)\n",
    "gs3.fit(X_train_rf,y_train_rf)\n",
    "print(gs3.best_estimator_)\n",
    "print(gs3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = RandomForestClassifier(max_depth=9, min_samples_split=12, n_estimators=250)\n",
    "best_rfc.fit(X_train_rf,y_train_rf)\n",
    "print(best_rfc.score(X_test_rf,y_test_rf))\n",
    "y_pred_rf = best_rfc.predict(X_test_rf)\n",
    "print(classification_report(y_test_rf, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "def RF_evaluate(n_estimators, min_samples_split, max_features, max_depth):\n",
    "    val = cross_val_score(\n",
    "            RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                       min_samples_split=int(min_samples_split),\n",
    "                       max_features=min(max_features, 0.999),\n",
    "                       max_depth=int(max_depth),\n",
    "                       random_state=90,\n",
    "                       n_jobs=-1),\n",
    "            X_train_rf, y_train_rf, scoring='f1', cv=5\n",
    "        ).mean()\n",
    "    return val\n",
    "\n",
    "pbounds = {'n_estimators': (50, 250), 'min_samples_split': (2, 25),'max_features': (0.1, 0.999),'max_depth': (5, 12)}\n",
    "\n",
    "RF_bo = BayesianOptimization(f=RF_evaluate, pbounds=pbounds, verbose=2,random_state=1,)\n",
    "\n",
    "RF_bo.maximize(init_points=5,n_iter=10,acq='ei')\n",
    "print(RF_bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = RandomForestClassifier(max_depth=10, max_features=0.93, min_samples_split=23, n_estimators=211)\n",
    "best_rfc.fit(X_train_rf,y_train_rf)\n",
    "print(best_rfc.score(X_test_rf,y_test_rf))\n",
    "y_pred_rf = best_rfc.predict(X_test_rf)\n",
    "print(classification_report(y_test_rf, y_pred_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
